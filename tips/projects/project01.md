# Project 1: Scraping, APIs, and Data Visualization

**Project Goals**

*   Introduce students to web scraping (and its challenges)
*   Introduce students to basic statistical analysis, how to use visual aids and the exploratory data analysis procedure.

**After the project, you will be able to:**

*   Use tools and libraries such as BeautifulSoup, Kimono, Nokogiri to collect structured data from the internet, or API wrappers that help pull structured data.
*   Store data in a DataFrame, and export it in some shape or form (text file, database, json text files)
*   Visualize data through means of Matplotlib or ggplot (the matplotlib wrapper)
*   Provide insight about the data in front of your peers

**Due Date and Grade Scale**

*   Projects will be due on April 23rd.

    *   You'll be presenting your projects in small groups on that class day, so come prepared
    *   Expect to see all projects turned in before the beginning of class
    *   If you get it to us earlier, Dave and I will try to provide early feedback

**Project Requirements**

*   Your are required to find a website with data to their interests, or one with api access
*   Good starts:

    *   [http://www.pythonapi.com/](http://www.pythonapi.com/)
    *   [https://www.kimonolabs.com/](https://www.kimonolabs.com/)
    *   [http://www.crummy.com/software/BeautifulSoup/](http://www.crummy.com/software/BeautifulSoup/)
    *   [http://nokogiri.org/](http://nokogiri.org/) for the rubyists in the classroom
    *   [https://apigee.com/console](https://apigee.com/console)

*   Write up a short piece about what is accessible through the api/web scraping:

    *   What fields (for the most part) are available?
    *   Write a question of interest using these fields, and generate a hypothesis
    *   Given the data's current structure, what changes will you have to make to it

*   Collect a manageable amount of data. Depending on your dataset, this could be anywhere between 5-50mb of data, or about 50k observations.
*   Prepare a short deck (5-6 slides) that cover the data that you scraped, what you wanted to do with it, and what you found interesting to a small group in class.

**You will Submit**

*   All code (language of their choice, but preferably Python)
*   All data (preferably compressed in a text file, or the python code should be able to reproduce the dataset)
*   Short deck in pdf format, including the short introductory piece.

**Going the Extra Mile**

*   You can (and should!) rework your project into a post on your own blog. It's encouraged to share your work within your social networks.
*   Engineers should consider exploring frameworks for deploying visuals on the web: d3.js, shiny (you'd also have to learn R, which is encouraged), iPython Notebook

**Lesson material in order to complete Project**

*   Fundamental Statistics Review
*   Python Basics and Using Libraries/Modules
*   Data collection, storage, and manipulation
*   Data visualization in Python

**Next Steps**

*   Consider how the data collected could be used in predictive analytics application
